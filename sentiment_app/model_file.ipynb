{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d063562",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import emoji\n",
    "#from pycontractions import Contractions\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from spellchecker import SpellChecker\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk.classify.util\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.classify import SklearnClassifier\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "import spacy\n",
    "from sklearn import feature_extraction, linear_model, model_selection, preprocessing\n",
    "# Download the spaCy model if not already present\n",
    "import sys\n",
    "import subprocess\n",
    "try:\n",
    "\tspacy.load(\"en_core_web_sm\")\n",
    "except OSError:\n",
    "\tsubprocess.run([sys.executable, \"-m\", \"spacy\", \"download\", \"en_core_web_sm\"])\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "#Load data\n",
    "\n",
    "#Train data\n",
    "twitter_data_train=pd.read_csv(\"/Users/r0g0aci/Documents/Personal/Python/sentiment_analysis/archive/Test.csv\")\n",
    "corpus= twitter_data_train['text'].tolist()\n",
    "labels= twitter_data_train['label'].tolist()\n",
    "\n",
    "#Cleaning text \n",
    "def clean_text(text):\n",
    "    if text is None or not isinstance(text, str):\n",
    "        return ''\n",
    "    text = text.lower()  # Lowercase\n",
    "    text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))  # Remove punctuation\n",
    "    text = re.sub(r'\\W', ' ', text)  # Remove special characters\n",
    "    text = BeautifulSoup(text, \"html.parser\").get_text()  # Remove HTML tags\n",
    "    text = emoji.demojize(text)\n",
    "    text = re.sub(r'@\\w+', '', text)  # Remove mentions\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)  # Remove URLs\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra spaces\n",
    "    #contractions = Contractions()\n",
    "    #contractions.load_models()\n",
    "    #text = contractions.expand_texts([text])[0]  # Expand contractions\n",
    "    spell = SpellChecker()\n",
    "    # Correct spelling\n",
    "    text = spell.correction(text)\n",
    "    return text\n",
    "\n",
    "#lemmatization\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "def word_lemmatize(text):\n",
    "    doc=nlp(text)\n",
    "    return [token.lemma_ for token in doc]\n",
    "\n",
    "# Create feature sets for training and testing\n",
    "count_vectorizer = feature_extraction.text.CountVectorizer(stop_words='english',ngram_range=(1,2), max_features=500, tokenizer=word_lemmatize)\n",
    "\n",
    "#Model - Naive Bayes Classifier\n",
    "model=make_pipeline(count_vectorizer,MultinomialNB())\n",
    "model.fit(corpus,labels)\n",
    "predicted=model.predict(corpus)\n",
    "print (\"Predictions:\", predicted[:10])\n",
    "print (\"Classification report:\\n\", classification_report(labels,predicted))\n",
    "\n",
    "# Save the model\n",
    "import pickle\n",
    "with open('/Users/r0g0aci/Documents/Personal/Python/sentiment_analysis/archives/sentiment_app/sentiment_classifier.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8bed7f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
